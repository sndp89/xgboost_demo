{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Created by NVIDIA CORPORATION\n",
    "#\n",
    "# Redistribution and use of this source code is governed by:\n",
    "# Apache License\n",
    "# Version 2.0, January 2004\n",
    "# http://www.apache.org/licenses/\n",
    "#\n",
    "# Contributors:\n",
    "# Ken Hester <khester@nvidia.com>\n",
    "# Source: NVIDIA RAPIDS\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Risk\n",
    "\n",
    "The example trains a model to perform home loan risk assessment using all of the loan data for the years 2000 to 2016 in the Fannie Mae loan performance dataset, consisting of roughly 400GB of data in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>\n",
    "## Contents\n",
    "__[Initialization](#initialize)__<br>\n",
    "__[Pandas DataFrame](#pandas_dataframe)__<br>\n",
    ">__[Model Training (CPU Only)](#pandas_model_training)__<br>\n",
    ">__[Model Training (Multi-GPU)](#pandas_gpu_model_training)__<br>\n",
    "\n",
    "__[cuDF DataFrame](#cudf_dataframe)__<br>\n",
    ">__[Model Training (Multi-GPU)](#cudf_gpu_model_training)__<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initialize'></a>\n",
    "## Initialize\n",
    "\n",
    "The mortgage dataset used for this demo: __[Fannie Mae Loan Dataset](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these commands in a bash shell to download and decompress the 1TB dataset into the data folder."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /rapids/data\n",
    "echo Downloading...\n",
    "\n",
    "\n",
    "echo Decompressing....\n",
    "tar -xzvf mortgage.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "#!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import numba.cuda as cuda\n",
    "import pandas as pd\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import cudf\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR = 2000\n",
    "END_YEAR = 2002 # end_year is not inclusive\n",
    "\n",
    "ACQ_DATA_PATH = \"/rapids/data/mortgage/acq\"\n",
    "PERF_DATA_PATH = \"/rapids/data/mortgage/perf\"\n",
    "COL_NAMES_PATH = \"/rapids/data/mortgage/names.csv\"\n",
    "\n",
    "PERCENT_TRAIN = 0.8\n",
    "NUM_ROUNDS = 100\n",
    "\n",
    "#The row stride to reduce the dataset for boosting\n",
    "#1: Every row: 100% of the data\n",
    "#2: Every 2nd row, 50% of the data\n",
    "#3: Every 3rd row, 33% of the data\n",
    "#etc\n",
    "PART_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_dataframe'></a>\n",
    "## Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(performance_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "            \n",
    "    return df  \n",
    "\n",
    "def load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    " \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "    \n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    df = pd.read_csv(acquisition_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "    return df      \n",
    "\n",
    "def load_names(col_names_path, **kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "\n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    df = pd.read_csv(col_names_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL and Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].codes.astype('int32').fillna(-1)\n",
    "\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column].fillna(-1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_12_mon_features(joined_df, **kwargs):\n",
    "    result_dfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        temp_df = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        temp_df['temp_months'] = temp_df['timestamp_year'] * 12 + temp_df['timestamp_month']\n",
    "        temp_df['temp_mody_n'] = np.floor((temp_df['temp_months'].astype('float64') - 24000 - y) / 12)\n",
    "        temp_df = temp_df.groupby(['loan_id', 'temp_mody_n']).agg({'delinquency_12': 'max','upb_12': 'min'}).reset_index()\n",
    "        temp_df.rename(columns={'delinquency_12':'max_delinquency_12','upb_12':'min_upb_12'}, inplace=True)\n",
    "        \n",
    "        temp_df['delinquency_12'] = (temp_df['max_delinquency_12']>3).astype('int32')\n",
    "        temp_df['delinquency_12'] +=(temp_df['min_upb_12']==0).astype('int32')\n",
    "        temp_df.drop('max_delinquency_12', axis=1, inplace=True)\n",
    "        temp_df['upb_12'] = temp_df['min_upb_12']\n",
    "        temp_df.drop('min_upb_12', axis=1, inplace=True)\n",
    "        temp_df['timestamp_year'] = np.floor(((temp_df['temp_mody_n'] * n_months) + 24000 + (y - 1)) / 12).astype('int16')\n",
    "        temp_df['timestamp_month'] = np.int8(y)\n",
    "        temp_df.drop('temp_mody_n', axis=1, inplace=True)\n",
    "        \n",
    "        result_dfs.append(temp_df)\n",
    "        del(temp_df)\n",
    "        \n",
    "    del(joined_df)\n",
    "    return pd.concat(result_dfs)\n",
    "\n",
    "def create_ever_features(df, **kwargs):\n",
    "    ever_df = df[['loan_id', 'current_loan_delinquency_status']]\n",
    "    ever_df = ever_df.groupby('loan_id').max().reset_index()\n",
    "    ever_df.rename(columns={'current_loan_delinquency_status':'max_current_loan_delinquency_status'}, inplace=True)\n",
    "    del(df)\n",
    "    \n",
    "    ever_df['ever_30'] = (ever_df['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    ever_df['ever_90'] = (ever_df['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    ever_df['ever_180'] = (ever_df['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    ever_df.drop('max_current_loan_delinquency_status', axis=1, inplace=True, errors = 'ignore')\n",
    "    return ever_df\n",
    "\n",
    "def create_delinq_features(df, **kwargs):\n",
    "    delinq_df = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(df)\n",
    "    \n",
    "    delinq_30 = delinq_df.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_30.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    \n",
    "    delinq_90 = delinq_df.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_90.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    \n",
    "    delinq_180 = delinq_df.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_180.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    delinq_merge_df = delinq_30.merge(delinq_90, how='left', on=['loan_id'])\n",
    "    delinq_merge_df['delinquency_90'] = delinq_merge_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge_df = delinq_merge_df.merge(delinq_180, how='left', on=['loan_id'])\n",
    "    delinq_merge_df['delinquency_180'] = delinq_merge_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge_df\n",
    "\n",
    "def join_ever_delinq_features(df, delinq_df, **kwargs):\n",
    "    ever_df = df.merge(delinq_df, on=['loan_id'], how='left')\n",
    "    del(df)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    ever_df['delinquency_30'] = ever_df['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_90'] = ever_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_180'] = ever_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return ever_df\n",
    "\n",
    "def create_joined_df(df, ever_df, **kwargs):\n",
    "    test = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(df)\n",
    "    \n",
    "    test['timestamp'] = pd.to_datetime(test['monthly_reporting_period'], infer_datetime_format=True)\n",
    "    test.drop('monthly_reporting_period', axis=1, inplace=True)\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop('current_loan_delinquency_status', axis=1, inplace=True)\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop('current_actual_upb', axis=1, inplace=True)\n",
    "    test['upb_12'].fillna(999999999, inplace=True)\n",
    "    test['delinquency_12'].fillna(-1, inplace=True)\n",
    "    \n",
    "    joined_df = test.merge(ever_df, how='left', on=['loan_id'])\n",
    "    del(ever_df)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'].fillna(-1, inplace=True)\n",
    "    joined_df['ever_90'].fillna(-1, inplace=True)\n",
    "    joined_df['ever_180'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_30'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_90'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_180'].fillna(-1, inplace=True)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop('delinquency_12', axis=1, inplace=True)\n",
    "    joined_df.drop('upb_12', axis=1, inplace=True)\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'])\n",
    "\n",
    "def final_performance_delinquency(df, joined_df, **kwargs):\n",
    "    merged_df = null_workaround(df)\n",
    "    joined_df = null_workaround(joined_df)\n",
    "    merged_df['timestamp_month'] = pd.to_datetime(merged_df['monthly_reporting_period'], infer_datetime_format=True).dt.month\n",
    "    merged_df['timestamp_month'] = merged_df['timestamp_month'].astype('int8')\n",
    "    merged_df['timestamp_year'] = pd.to_datetime(merged_df['monthly_reporting_period'], infer_datetime_format=True).dt.year\n",
    "    merged_df['timestamp_year'] = merged_df['timestamp_year'].astype('int16')\n",
    "    merged_df = merged_df.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'])\n",
    "    merged_df.drop('timestamp_year', axis=1, inplace=True)\n",
    "    merged_df.drop('timestamp_month', axis=1, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def join_perf_acq_gdfs(perf_df, acq_df, **kwargs):\n",
    "    perf_df = null_workaround(perf_df)\n",
    "    acq_df = null_workaround(acq_df)\n",
    "    return perf_df.merge(acq_df, how='left', on=['loan_id'])\n",
    "\n",
    "def last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    #for column in drop_list:\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_quarter(names_df, acq_df, perf_df):\n",
    "    acq_df = acq_df.merge(names_df, how='left', on=['seller_name'])\n",
    "    acq_df.drop('seller_name', axis=1, inplace=True)\n",
    "    acq_df['seller_name'] = acq_df['new']\n",
    "    acq_df.drop('new', axis=1, inplace=True)\n",
    "        \n",
    "    df = perf_df\n",
    "    ever_df = create_ever_features(df)\n",
    "    delinq_merge = create_delinq_features(df)\n",
    "    ever_df = join_ever_delinq_features(ever_df, delinq_merge)\n",
    "    del(delinq_merge)\n",
    " \n",
    "    joined_df = create_joined_df(df, ever_df)\n",
    "    test_df = create_12_mon_features(joined_df)\n",
    "    joined_df = combine_joined_12_mon(joined_df, test_df)\n",
    "    del(test_df)\n",
    "    \n",
    "    perf_df = final_performance_delinquency(df, joined_df)\n",
    "    del(df, joined_df)\n",
    "    \n",
    "    final_df = join_perf_acq_gdfs(perf_df, acq_df)\n",
    "    del(perf_df)\n",
    "    del(acq_df)\n",
    "  \n",
    "    final_df = last_mile_cleaning(final_df)\n",
    "    return final_df\n",
    "\n",
    "def process_quarter_file(acquisition_file, perf_file):\n",
    "    print(\"Processing file: {0}\".format(perf_file))\n",
    "    \n",
    "    #Load Data\n",
    "    names_df = load_names(COL_NAMES_PATH)\n",
    "    acq_df = load_acquisition_csv(acquisition_file)\n",
    "    perf_df = load_performance_csv(perf_file)\n",
    "    \n",
    "    df = process_quarter(names_df, acq_df, perf_df)\n",
    "    del(acq_df, perf_df, names_df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#read in data files onr at a time because pandas performance does not improve with multi-threading\n",
    "frames = []\n",
    "quarter = 1\n",
    "year = START_YEAR\n",
    "while year != END_YEAR:\n",
    "    acquisition_file = ACQ_DATA_PATH + \"/Acquisition_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n",
    "    files = glob(os.path.join(PERF_DATA_PATH + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\"))\n",
    "    for file in files:\n",
    "        df1 = process_quarter_file(acquisition_file, file)\n",
    "        frames.append(df1) \n",
    "\n",
    "    quarter += 1\n",
    "    if quarter == 5:\n",
    "        year += 1\n",
    "        quarter = 1\n",
    "        \n",
    "print(\"Concatenating dataframes\")\n",
    "df_train = pd.concat(frames)    \n",
    "del frames\n",
    "\n",
    "#Optional: Review the data and data types\n",
    "#print (df_train.head())\n",
    "#print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the data by the desired columns\n",
    "\n",
    "PRED_VARS_X = df_train.columns.difference(['delinquency_12'])\n",
    "PRED_VARS_Y ='delinquency_12'\n",
    "\n",
    "x = df_train[PRED_VARS_X]\n",
    "y = df_train[PRED_VARS_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = int(int(len(df_train)) * PERCENT_TRAIN)\n",
    "print(rows_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "x_train = x[:rows_train:PART_COUNT]\n",
    "x_test = x[rows_train::PART_COUNT]\n",
    "y_train = y[:rows_train:PART_COUNT]\n",
    "y_test = y[rows_train::PART_COUNT]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_model_training'></a>\n",
    "### Model Training\n",
    "CPU XGBoost using the *fit* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "#Optional: loop over CPU threads for sizing and performance\n",
    "NTHREADS=multiprocessing.cpu_count()\n",
    "\n",
    "print(\"threads: {0}\".format(NTHREADS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators = NUM_ROUNDS,\n",
    "    gamma = 1,\n",
    "    eta=0.1,\n",
    "    alpha=0.9,\n",
    "    subsample=1,\n",
    "    reg_lambda = 1,\n",
    "    scale_pos_weight = 2,\n",
    "    min_child_weight = 30,    \n",
    "    max_depth = 8,\n",
    "    early_stopping_rounds = 5,\n",
    "    max_features = 'auto',\n",
    "    max_leaves = 2**8,\n",
    "    objective = 'reg:linear',\n",
    "    booster = 'gbtree',\n",
    "    criterion = 'friedman_mse',\n",
    "    grow_policy = 'lossguide',\n",
    "    tree_method = 'hist',\n",
    "    nthread = NTHREADS,\n",
    "    silent = True)\n",
    "\n",
    "gbm = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy (AUC) is measured by the area under the ROC curve. The definition of an acceptable AUC is relative and not absolute based on the data and feature. An area of 1 represents a perfect test; an area of .5 represents a poor or unrealiable test. A suggested rough guide for classifying the accuracy of a XGBoost:<br>\n",
    ">90 - 100 = Excellent <br>\n",
    ">80 - 90 = Good <br>\n",
    ">70 - 80 = Fair <br>\n",
    ">60 - 70 = Poor <br>\n",
    ">50 - 60 = Fail <br>\n",
    ">50 and Below = Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = gbm.predict(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "auc = metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "acc = metrics.roc_auc_score(y_test, pred) * 100\n",
    "\n",
    "print(\"AUC: {0}, ROC AUC: {1}\".format(auc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup for the next iteration\n",
    "del fpr\n",
    "del tpr\n",
    "del thresholds \n",
    "del acc\n",
    "del auc\n",
    "del pred\n",
    "\n",
    "del model\n",
    "del gbm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional iterations, goto top of [model training section](#pandas_model_training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_gpu_model_training'></a>\n",
    "### Model Training\n",
    "GPU accelerated XGBoost using the *fit* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use all GPUs, set NGPUS=-1\n",
    "#Optional: loop over number of GPUs for sizing and performance\n",
    "NGPUS=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators = NUM_ROUNDS,\n",
    "    gamma = 1,\n",
    "    eta=0.1,\n",
    "    alpha=0.9,\n",
    "    subsample=1,\n",
    "    reg_lambda = 1,\n",
    "    scale_pos_weight = 2,\n",
    "    min_child_weight = 30,    \n",
    "    max_depth = 8,\n",
    "    early_stopping_rounds = 5,\n",
    "    max_features = 'auto',\n",
    "    max_leaves = 2**8,\n",
    "    objective = 'gpu:reg:linear',\n",
    "    booster = 'gbtree',\n",
    "    criterion = 'friedman_mse',\n",
    "    grow_policy = 'lossguide',\n",
    "    tree_method = 'gpu_hist',\n",
    "    n_gpus = NGPUS,\n",
    "    silent = True)\n",
    "\n",
    "gbm = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy (AUC) is measured by the area under the ROC curve. The definition of an acceptable AUC is relative and not absolute based on the data and feature. An area of 1 represents a perfect test; an area of .5 represents a poor or unrealiable test. A suggested rough guide for classifying the accuracy of a XGBoost:<br>\n",
    ">90 - 100 = Excellent <br>\n",
    ">80 - 90 = Good <br>\n",
    ">70 - 80 = Fair <br>\n",
    ">60 - 70 = Poor <br>\n",
    ">50 - 60 = Fail <br>\n",
    ">50 and Below = Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = gbm.predict(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "auc = metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "acc = metrics.roc_auc_score(y_test, pred) * 100\n",
    "\n",
    "print(\"AUC: {0}, ROC AUC: {1}\".format(auc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup for the next iteration\n",
    "del fpr\n",
    "del tpr\n",
    "del thresholds \n",
    "del acc\n",
    "del auc\n",
    "del pred\n",
    "\n",
    "del model\n",
    "del gbm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional iterations, goto top of [model training section](#pandas_gpu_model_training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After looping through the thead counts, cleanup for the benchmark\n",
    "del x\n",
    "del y\n",
    "\n",
    "del x_train\n",
    "del x_test \n",
    "del y_train \n",
    "del y_test\n",
    "\n",
    "del df_train\n",
    "#del df_validate\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cudf_dataframe'></a>\n",
    "## cuDF DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def cudf_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    return cudf.read_csv(performance_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def cudf_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    " \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "    \n",
    "    return cudf.read_csv(acquisition_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def cudf_load_names(col_names_path, **kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "\n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    return cudf.read_csv(col_names_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL and Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cudf_null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column] = df[column].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def cudf_create_12_mon_features(joined_df, **kwargs):\n",
    "    result_dfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        temp_df = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        temp_df['temp_months'] = temp_df['timestamp_year'] * 12 + temp_df['timestamp_month']\n",
    "        temp_df['temp_mody_n'] = ((temp_df['temp_months'].astype('float64') - 24000 - y) / 12).floor()\n",
    "        temp_df = temp_df.groupby(['loan_id', 'temp_mody_n'], method='hash').agg({'delinquency_12': 'max','upb_12': 'min'})\n",
    "        temp_df['delinquency_12'] = (temp_df['max_delinquency_12']>3).astype('int32')\n",
    "        temp_df['delinquency_12'] +=(temp_df['min_upb_12']==0).astype('int32')\n",
    "        temp_df.drop_column('max_delinquency_12')\n",
    "        temp_df['upb_12'] = temp_df['min_upb_12']\n",
    "        temp_df.drop_column('min_upb_12')\n",
    "        temp_df['timestamp_year'] = (((temp_df['temp_mody_n'] * n_months) + 24000 + (y - 1)) / 12).floor().astype('int16')\n",
    "        temp_df['timestamp_month'] = np.int8(y)\n",
    "        temp_df.drop_column('temp_mody_n')\n",
    "        \n",
    "        result_dfs.append(temp_df)\n",
    "        del(temp_df)\n",
    "        \n",
    "    del(joined_df)\n",
    "    return cudf.concat(result_dfs)\n",
    "\n",
    "def cudf_create_ever_features(df, **kwargs):\n",
    "    ever_df = df[['loan_id', 'current_loan_delinquency_status']]\n",
    "    ever_df = ever_df.groupby('loan_id', method='hash').max()\n",
    "    del(df)\n",
    "    \n",
    "    ever_df['ever_30'] = (ever_df['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    ever_df['ever_90'] = (ever_df['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    ever_df['ever_180'] = (ever_df['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    ever_df.drop_column('max_current_loan_delinquency_status')\n",
    "    return ever_df\n",
    "\n",
    "def cudf_create_delinq_features(df, **kwargs):\n",
    "    delinq_df = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(df)\n",
    "    \n",
    "    delinq_30 = delinq_df.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop_column('min_monthly_reporting_period')\n",
    "    delinq_90 = delinq_df.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop_column('min_monthly_reporting_period')\n",
    "    delinq_180 = delinq_df.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop_column('min_monthly_reporting_period')\n",
    "    del(delinq_df)\n",
    "    \n",
    "    delinq_merge_df = delinq_30.merge(delinq_90, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge_df['delinquency_90'] = delinq_merge_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge_df = delinq_merge_df.merge(delinq_180, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge_df['delinquency_180'] = delinq_merge_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge_df\n",
    "\n",
    "def cudf_join_ever_delinq_features(df, delinq_df, **kwargs):\n",
    "    ever_df = df.merge(delinq_df, on=['loan_id'], how='left', type='hash')\n",
    "    del(df)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    ever_df['delinquency_30'] = ever_df['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_90'] = ever_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_180'] = ever_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return ever_df\n",
    "\n",
    "def cudf_create_joined_df(df, ever_df, **kwargs):\n",
    "    test = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(df)\n",
    "    \n",
    "    test['timestamp'] = test['monthly_reporting_period']\n",
    "    test.drop_column('monthly_reporting_period')\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop_column('current_loan_delinquency_status')\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop_column('current_actual_upb')\n",
    "    test['upb_12'] = test['upb_12'].fillna(999999999)\n",
    "    test['delinquency_12'] = test['delinquency_12'].fillna(-1)\n",
    "    \n",
    "    joined_df = test.merge(ever_df, how='left', on=['loan_id'], type='hash')\n",
    "    del(ever_df)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'] = joined_df['ever_30'].fillna(-1)\n",
    "    joined_df['ever_90'] = joined_df['ever_90'].fillna(-1)\n",
    "    joined_df['ever_180'] = joined_df['ever_180'].fillna(-1)\n",
    "    joined_df['delinquency_30'] = joined_df['delinquency_30'].fillna(-1)\n",
    "    joined_df['delinquency_90'] = joined_df['delinquency_90'].fillna(-1)\n",
    "    joined_df['delinquency_180'] = joined_df['delinquency_180'].fillna(-1)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def cudf_combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop_column('delinquency_12')\n",
    "    joined_df.drop_column('upb_12')\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    df = joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "    return df\n",
    "\n",
    "def cudf_final_performance_delinquency(df, joined_df, **kwargs):\n",
    "    merged_df = cudf_null_workaround(df)\n",
    "    joined_df = cudf_null_workaround(joined_df)\n",
    "    merged_df['timestamp_month'] = merged_df['monthly_reporting_period'].dt.month\n",
    "    merged_df['timestamp_month'] = merged_df['timestamp_month'].astype('int8')\n",
    "    merged_df['timestamp_year'] = merged_df['monthly_reporting_period'].dt.year\n",
    "    merged_df['timestamp_year'] = merged_df['timestamp_year'].astype('int16')\n",
    "    merged_df = merged_df.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "    merged_df.drop_column('timestamp_year')\n",
    "    merged_df.drop_column('timestamp_month')\n",
    "    return merged_df\n",
    "\n",
    "def cudf_join_perf_acq_gdfs(perf_df, acq_df, **kwargs):\n",
    "    perf_df = cudf_null_workaround(perf_df)\n",
    "    acq_df = cudf_null_workaround(acq_df)\n",
    "    merge_df = perf_df.merge(acq_df, how='left', on=['loan_id'], type='hash')\n",
    "    return merge_df\n",
    "\n",
    "def cudf_last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    for column in drop_list:\n",
    "        df.drop_column(column)\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cudf_process_quarter(names_df, acq_df, perf_df):\n",
    "    acq_df = acq_df.merge(names_df, how='left', on=['seller_name'], type='hash')\n",
    "    acq_df.drop_column('seller_name')\n",
    "    acq_df['seller_name'] = acq_df['new']\n",
    "    acq_df.drop_column('new')\n",
    "    \n",
    "    df = perf_df\n",
    "    ever_df = cudf_create_ever_features(df)\n",
    "    delinq_merge = cudf_create_delinq_features(df)\n",
    "    ever_df = cudf_join_ever_delinq_features(ever_df, delinq_merge)\n",
    "    del(delinq_merge)\n",
    "    \n",
    "    joined_df = cudf_create_joined_df(df, ever_df)\n",
    "    test_df = cudf_create_12_mon_features(joined_df)\n",
    "    joined_df = cudf_combine_joined_12_mon(joined_df, test_df)\n",
    "    del(test_df)\n",
    "\n",
    "    perf_df = cudf_final_performance_delinquency(df, joined_df)\n",
    "    del(df, joined_df)\n",
    "\n",
    "    final_df = cudf_join_perf_acq_gdfs(perf_df, acq_df)\n",
    "    del(perf_df)\n",
    "    del(acq_df)\n",
    "\n",
    "    final_df = cudf_last_mile_cleaning(final_df)\n",
    "    return final_df\n",
    "\n",
    "def cudf_process_quarter_file(acquisition_file, perf_file):\n",
    "    #Load Data\n",
    "    names_df = cudf_load_names(COL_NAMES_PATH)\n",
    "    acq_df = cudf_load_acquisition_csv(acquisition_file)\n",
    "    perf_df = cudf_load_performance_csv(perf_file)\n",
    "\n",
    "    df = cudf_process_quarter(names_df, acq_df, perf_df)\n",
    "    return df\n",
    "\n",
    "def process_quarter_file_multigpu(acquisition_file, perf_file, gpuid=0):\n",
    "    print(\"On GPU {0}, Processing file: {1}\".format(gpuid, perf_file))\n",
    "    cuda.select_device(int(gpuid))\n",
    "    \n",
    "    #Load Data\n",
    "    df = cudf_process_quarter_file(acquisition_file, perf_file)\n",
    "    return df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cudf_gpu_model_training'></a>\n",
    "### Model Training (Multi-GPU)\n",
    "GPU accelerated XGBoost using the *train* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import os\n",
    "import concurrent.futures\n",
    "import numba.cuda as cuda\n",
    "\n",
    "def process_quarters_multigpu(start_year, end_year, ngpus=len(cuda.gpus)):\n",
    "    #Read the csv file(s)\n",
    "    future_files = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=ngpus) as executor:\n",
    "        i = 0\n",
    "        gpuid = 0\n",
    "        quarter = 1\n",
    "        year = start_year\n",
    "        while year != end_year:\n",
    "            acquisition_file = ACQ_DATA_PATH + \"/Acquisition_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n",
    "            files = glob(os.path.join(PERF_DATA_PATH + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\"))\n",
    "            for file in files:\n",
    "                future_file = executor.submit(process_quarter_file_multigpu,acquisition_file, file, gpuid)\n",
    "                future_files[future_file] = file\n",
    "                i = i + 1\n",
    "                #Creates a gpuid from range 0:NUM_GPUS-1\n",
    "                gpuid = i % ngpus\n",
    "\n",
    "            quarter += 1\n",
    "            if quarter == 5:\n",
    "                year += 1\n",
    "                quarter = 1\n",
    "\n",
    "    frames = []\n",
    "    for future in concurrent.futures.as_completed(future_files):\n",
    "        file = future_files[future]\n",
    "        try:\n",
    "            df1 = future.result()\n",
    "        except Exception as exc:\n",
    "            print('{0} generated an exception: {1}'.format(file, exc))\n",
    "        else:\n",
    "            #print('{0} file is {1} bytes'.format(file, len(df1)))\n",
    "            frames.append(df1)\n",
    "\n",
    "    print(\"Concatenating dataframes\")\n",
    "    df = pd.concat(frames)    \n",
    "    del frames\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#read in data files using multi-gpu\n",
    "df_train = process_quarters_multigpu(START_YEAR, END_YEAR)\n",
    "\n",
    "#Optional: Review the data and data types\n",
    "#print (gdf_train.head())\n",
    "#print(gdf_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#filter the data by the desired columns\n",
    "\n",
    "PRED_VARS_X = gdf_train.columns.difference(['delinquency_12'])\n",
    "PRED_VARS_Y ='delinquency_12'\n",
    "\n",
    "x = df_train[PRED_VARS_X]\n",
    "y = df_train[PRED_VARS_Y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = int(int(len(df_train)) * PERCENT_TRAIN)\n",
    "print(rows_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "x_train = x[:rows_train:PART_COUNT]\n",
    "x_test = x[rows_train::PART_COUNT]\n",
    "y_train = y[:rows_train:PART_COUNT]\n",
    "y_test = y[rows_train::PART_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Basic:\n",
    "#dtrain = xgb.DMatrix(gx_train, label=gy_train)\n",
    "#dtest = xgb.DMatrix(gx_test)\n",
    "##Intermediate: This code executes around 1.8x times faster than basic\n",
    "dtrain = xgb.DMatrix(x_train.values, label=y_train.values, nthread=-1)\n",
    "dtest = xgb.DMatrix(x_test.values, nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#To use all GPUs, set NGPUS=-1\n",
    "#Optional: loop over number of GPUs for sizing and performance\n",
    "NGPUS=-1\n",
    "\n",
    "params={}\n",
    "params['learning_rate'] = 0.1\n",
    "params['n_estimators'] = NUM_ROUNDS\n",
    "params['gamma'] = 1\n",
    "params['eta'] = 0.1\n",
    "params['alpha'] = 0.9\n",
    "params['subsample'] = 1\n",
    "params['reg_lambda'] = 1\n",
    "params['scale_pos_weight'] = 2\n",
    "params['min_child_weight'] = 30\n",
    "params['max_depth'] = 8\n",
    "params['min_child_weight'] = 3\n",
    "params['early_stopping_rounds'] = 5\n",
    "params['max_features'] = 'auto'\n",
    "params['max_leaves'] = 2**8\n",
    "params['objective'] = 'gpu:reg:linear'\n",
    "params['booster'] = 'gbtree'\n",
    "params['criterion'] = 'friedman_mse'\n",
    "params['grow_policy'] = 'lossguide'\n",
    "params['tree_method'] = 'gpu_hist'\n",
    "params['n_gpus'] = NGPUS\n",
    "params['silent'] = True\n",
    "\n",
    "#evallist = [(dtest, 'validation'),(dtrain, 'train')]\n",
    "evallist = [(dtrain, 'x_train')]\n",
    "gbm = xgb.train(params, dtrain, NUM_ROUNDS, evallist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy (AUC) is measured by the area under the ROC curve. The definition of an acceptable AUC is relative and not absolute based on the data and feature. An area of 1 represents a perfect test; an area of .5 represents a poor or unrealiable test. A suggested rough guide for classifying the accuracy of a XGBoost:<br>\n",
    ">90 - 100 = Excellent <br>\n",
    ">80 - 90 = Good <br>\n",
    ">70 - 80 = Fair <br>\n",
    ">60 - 70 = Poor <br>\n",
    ">50 - 60 = Fail <br>\n",
    ">50 and Below = Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = gbm.predict(dtest, ntree_limit=NUM_ROUNDS)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "auc = metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "acc = metrics.roc_auc_score(y_test, pred) * 100\n",
    "\n",
    "print(\"AUC: {0}, ROC AUC: {1}\".format(auc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup for the next iteration\n",
    "del acc\n",
    "del auc\n",
    "del fpr\n",
    "del tpr\n",
    "del thresholds\n",
    "del pred\n",
    "\n",
    "del gbm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional iterations, goto top of [model training section](#cudf_gpu_model_training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After looping through the gpus, cleanup for the benchmark\n",
    "del dtrain\n",
    "del dtest\n",
    "\n",
    "del x\n",
    "del y\n",
    "\n",
    "del x_train\n",
    "del x_test \n",
    "del y_train \n",
    "del y_test \n",
    "\n",
    "#cuDF Dataframe\n",
    "del df_train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleanup'></a>\n",
    "## Cleanup / Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
