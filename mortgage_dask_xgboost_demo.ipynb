{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Created by NVIDIA CORPORATION\n",
    "#\n",
    "# Redistribution and use of this source code is governed by:\n",
    "# Apache License\n",
    "# Version 2.0, January 2004\n",
    "# http://www.apache.org/licenses/\n",
    "#\n",
    "# Contributors:\n",
    "# Ken Hester <khester@nvidia.com>\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Risk\n",
    "\n",
    "The example trains a model to perform home loan risk assessment using all of the loan data for the years 2000 to 2016 in the Fannie Mae loan performance dataset, consisting of roughly 400GB of data in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>\n",
    "## Contents\n",
    "__[Initialization](#initialize)__<br>\n",
    "__[Pandas Dask DataFrame](#pandas_dataframe)__<br>\n",
    ">__[Model Training (Multi-core)](#dask_pandas_model_training)__<br>\n",
    "\n",
    "__[cuDF Dask DataFrame](#cudf_dataframe)__<br>\n",
    ">__[Model Training (Multi-GPU)](#dask_cudf_model_training)__<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initialize'></a>\n",
    "## Initialize\n",
    "\n",
    "The mortgage dataset used for this demo: __[Fannie Mae Loan Dataset](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these commands in a bash shell to download and decompress the 1TB dataset into the data folder."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /rapids/data\n",
    "echo Downloading...\n",
    "\n",
    "\n",
    "echo Decompressing....\n",
    "tar -xzvf mortgage.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "#!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import numba.cuda as cuda\n",
    "import pandas as pd\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import cudf\n",
    "import gc\n",
    "\n",
    "#Dask \n",
    "import dask_xgboost as dxgb\n",
    "import dask\n",
    "import dask_cudf\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR = 2000\n",
    "END_YEAR = 2002 # end_year is not inclusive\n",
    "\n",
    "ACQ_DATA_PATH = \"/rapids/data/mortgage/acq\"\n",
    "PERF_DATA_PATH = \"/rapids/data/mortgage/perf\"\n",
    "COL_NAMES_PATH = \"/rapids/data/mortgage/names.csv\"\n",
    "\n",
    "PERCENT_TRAIN = 0.8\n",
    "NUM_ROUNDS = 100\n",
    "\n",
    "DASK_UTILS_PATH = \"/rapids/notebooks/utils\"\n",
    "#The row stride to reduce the dataset for boosting, and increase the total number of datasets by part_count.\n",
    "#All the rows in the dataset are processed, just in a smaller or larger chunks.\n",
    "#1: Every row: 100% of the data\n",
    "#2: Every 2nd row, 50% of the data\n",
    "#3: Every 3rd row, 33% of the data\n",
    "#etc\n",
    "PART_COUNT = 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numba.cuda as cuda\n",
    "import multiprocessing\n",
    "\n",
    "#Initialize variables\n",
    "_is_dask_initialized = False\n",
    "_dask_client = None\n",
    "\n",
    "def dask_initialize(nworkers=-1, ngpus=-1):\n",
    "    global _is_dask_initialized\n",
    "    global _dask_client\n",
    "    \n",
    "    if _is_dask_initialized == True:\n",
    "        return _dask_client\n",
    "    \n",
    "    if (nworkers==-1):\n",
    "        nworkers=multiprocessing.cpu_count()\n",
    "    \n",
    "    if (ngpus==-1):\n",
    "        ngpus=len(cuda.gpus)\n",
    "    \n",
    "    #read in data files using 1 dask worker per gpu for multi-gpu processing\n",
    "    if (ngpus > 0):\n",
    "        nworkers=ngpus\n",
    "        \n",
    "    #Assume all node have the same configuration\n",
    "    IPADDR = subprocess.check_output(['hostname','--all-ip-addresses'])\n",
    "    IPADDR = IPADDR.decode('UTF-8').split()[0]\n",
    "\n",
    "    bash_command_1 = \"{0}/dask-setup.sh rapids 0,0\".format(DASK_UTILS_PATH)\n",
    "    bash_command_2 = \"{0}/dask-setup.sh rapids {1},{2} 8786 8787 8790 {3} MASTER\".format(DASK_UTILS_PATH, nworkers, ngpus, IPADDR)\n",
    "\n",
    "    print(bash_command_1)\n",
    "    output = subprocess.check_output(['bash','-c', bash_command_1])\n",
    "    print(bash_command_2)\n",
    "    output = subprocess.check_output(['bash','-c', bash_command_2])\n",
    "    \n",
    "    ipaddr_client = IPADDR + str(\":8786\")\n",
    "    \n",
    "    _dask_client = dask.distributed.Client(ipaddr_client)\n",
    "    _is_dask_initialized = True\n",
    "    return _dask_client\n",
    "\n",
    "def dask_release():\n",
    "    global _is_dask_initialized\n",
    "    global _dask_client\n",
    "    \n",
    "    bash_command_1 = \"{0}/dask-setup.sh rapids 0,0\".format(DASK_UTILS_PATH)\n",
    "    print(bash_command_1)\n",
    "    output = subprocess.check_output(['bash','-c', bash_command_1])\n",
    "    \n",
    "    _is_dask_initialized = False\n",
    "    del(_dask_client)\n",
    "    _dask_client = None\n",
    "\n",
    "def dask_run_task(func, **kwargs):\n",
    "    task = func(**kwargs)\n",
    "    return task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmm_initialize():\n",
    "    from librmm_cffi import librmm_config as rmm_cfg\n",
    "\n",
    "    rmm_cfg.use_pool_allocator = True\n",
    "    #rmm_cfg.initial_pool_size = 2<<30 # set to 2GiB. Default is 1/2 total GPU memory\n",
    "    import cudf\n",
    "    return cudf._gdf.rmm_initialize()\n",
    "\n",
    "def rmm_initialize_nopool():\n",
    "    from librmm_cffi import librmm_config as rmm_cfg\n",
    "\n",
    "    rmm_cfg.use_pool_allocator = False\n",
    "    #rmm_cfg.initial_pool_size = 2<<30 # set to 2GiB. Default is 1/2 total GPU memory\n",
    "    import cudf\n",
    "    return cudf._gdf.rmm_initialize()\n",
    "\n",
    "def rmm_release():\n",
    "    return cudf._gdf.rmm_finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas_dataframe'></a>\n",
    "## Pandas Dask Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(performance_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "            \n",
    "    return df  \n",
    "\n",
    "def load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    " \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "    \n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    df = pd.read_csv(acquisition_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "    return df      \n",
    "\n",
    "def load_names(col_names_path, **kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "\n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    #dtype was not pass into read_csv, so convert/create categorical fields \n",
    "    df = pd.read_csv(col_names_path, names=cols, delimiter='|', skiprows=1)\n",
    "    \n",
    "    for c in dtypes: \n",
    "        if (dtypes[c] == \"category\"):\n",
    "            df[c] = pd.Categorical(df[c]).codes          \n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL and Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].codes.astype('int32').fillna(-1)\n",
    "\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column].fillna(-1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_12_mon_features(joined_df, **kwargs):\n",
    "    result_dfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        temp_df = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        temp_df['temp_months'] = temp_df['timestamp_year'] * 12 + temp_df['timestamp_month']\n",
    "        temp_df['temp_mody_n'] = np.floor((temp_df['temp_months'].astype('float64') - 24000 - y) / 12)\n",
    "        temp_df = temp_df.groupby(['loan_id', 'temp_mody_n']).agg({'delinquency_12': 'max','upb_12': 'min'}).reset_index()\n",
    "        temp_df.rename(columns={'delinquency_12':'max_delinquency_12','upb_12':'min_upb_12'}, inplace=True)\n",
    "        \n",
    "        temp_df['delinquency_12'] = (temp_df['max_delinquency_12']>3).astype('int32')\n",
    "        temp_df['delinquency_12'] +=(temp_df['min_upb_12']==0).astype('int32')\n",
    "        temp_df.drop('max_delinquency_12', axis=1, inplace=True)\n",
    "        temp_df['upb_12'] = temp_df['min_upb_12']\n",
    "        temp_df.drop('min_upb_12', axis=1, inplace=True)\n",
    "        temp_df['timestamp_year'] = np.floor(((temp_df['temp_mody_n'] * n_months) + 24000 + (y - 1)) / 12).astype('int16')\n",
    "        temp_df['timestamp_month'] = np.int8(y)\n",
    "        temp_df.drop('temp_mody_n', axis=1, inplace=True)\n",
    "        \n",
    "        result_dfs.append(temp_df)\n",
    "        del(temp_df)\n",
    "        \n",
    "    del(joined_df)\n",
    "    return pd.concat(result_dfs)\n",
    "\n",
    "def create_ever_features(df, **kwargs):\n",
    "    ever_df = df[['loan_id', 'current_loan_delinquency_status']]\n",
    "    ever_df = ever_df.groupby('loan_id').max().reset_index()\n",
    "    ever_df.rename(columns={'current_loan_delinquency_status':'max_current_loan_delinquency_status'}, inplace=True)\n",
    "    del(df)\n",
    "    \n",
    "    ever_df['ever_30'] = (ever_df['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    ever_df['ever_90'] = (ever_df['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    ever_df['ever_180'] = (ever_df['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    ever_df.drop('max_current_loan_delinquency_status', axis=1, inplace=True, errors = 'ignore')\n",
    "    return ever_df\n",
    "\n",
    "def create_delinq_features(df, **kwargs):\n",
    "    delinq_df = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(df)\n",
    "    \n",
    "    delinq_30 = delinq_df.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_30.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    \n",
    "    delinq_90 = delinq_df.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_90.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    \n",
    "    delinq_180 = delinq_df.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min().reset_index()\n",
    "    delinq_180.rename(columns={'monthly_reporting_period':'min_monthly_reporting_period'}, inplace=True)\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop('min_monthly_reporting_period', axis=1, inplace=True)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    delinq_merge_df = delinq_30.merge(delinq_90, how='left', on=['loan_id'])\n",
    "    delinq_merge_df['delinquency_90'] = delinq_merge_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge_df = delinq_merge_df.merge(delinq_180, how='left', on=['loan_id'])\n",
    "    delinq_merge_df['delinquency_180'] = delinq_merge_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge_df\n",
    "\n",
    "def join_ever_delinq_features(df, delinq_df, **kwargs):\n",
    "    ever_df = df.merge(delinq_df, on=['loan_id'], how='left')\n",
    "    del(df)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    ever_df['delinquency_30'] = ever_df['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_90'] = ever_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_180'] = ever_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return ever_df\n",
    "\n",
    "def create_joined_df(df, ever_df, **kwargs):\n",
    "    test = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(df)\n",
    "    \n",
    "    test['timestamp'] = pd.to_datetime(test['monthly_reporting_period'], infer_datetime_format=True)\n",
    "    test.drop('monthly_reporting_period', axis=1, inplace=True)\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop('current_loan_delinquency_status', axis=1, inplace=True)\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop('current_actual_upb', axis=1, inplace=True)\n",
    "    test['upb_12'].fillna(999999999, inplace=True)\n",
    "    test['delinquency_12'].fillna(-1, inplace=True)\n",
    "    \n",
    "    joined_df = test.merge(ever_df, how='left', on=['loan_id'])\n",
    "    del(ever_df)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'].fillna(-1, inplace=True)\n",
    "    joined_df['ever_90'].fillna(-1, inplace=True)\n",
    "    joined_df['ever_180'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_30'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_90'].fillna(-1, inplace=True)\n",
    "    joined_df['delinquency_180'].fillna(-1, inplace=True)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop('delinquency_12', axis=1, inplace=True)\n",
    "    joined_df.drop('upb_12', axis=1, inplace=True)\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'])\n",
    "\n",
    "def final_performance_delinquency(df, joined_df, **kwargs):\n",
    "    merged_df = null_workaround(df)\n",
    "    joined_df = null_workaround(joined_df)\n",
    "    merged_df['timestamp_month'] = pd.to_datetime(merged_df['monthly_reporting_period'], infer_datetime_format=True).dt.month\n",
    "    merged_df['timestamp_month'] = merged_df['timestamp_month'].astype('int8')\n",
    "    merged_df['timestamp_year'] = pd.to_datetime(merged_df['monthly_reporting_period'], infer_datetime_format=True).dt.year\n",
    "    merged_df['timestamp_year'] = merged_df['timestamp_year'].astype('int16')\n",
    "    merged_df = merged_df.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'])\n",
    "    merged_df.drop('timestamp_year', axis=1, inplace=True)\n",
    "    merged_df.drop('timestamp_month', axis=1, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def join_perf_acq_gdfs(perf_df, acq_df, **kwargs):\n",
    "    perf_df = null_workaround(perf_df)\n",
    "    acq_df = null_workaround(acq_df)\n",
    "    return perf_df.merge(acq_df, how='left', on=['loan_id'])\n",
    "\n",
    "def last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    #for column in drop_list:\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_quarter(names_df, acq_df, perf_df):\n",
    "    acq_df = acq_df.merge(names_df, how='left', on=['seller_name'])\n",
    "    acq_df.drop('seller_name', axis=1, inplace=True)\n",
    "    acq_df['seller_name'] = acq_df['new']\n",
    "    acq_df.drop('new', axis=1, inplace=True)\n",
    "        \n",
    "    df = perf_df\n",
    "    ever_df = create_ever_features(df)\n",
    "    delinq_merge = create_delinq_features(df)\n",
    "    ever_df = join_ever_delinq_features(ever_df, delinq_merge)\n",
    "    del(delinq_merge)\n",
    " \n",
    "    joined_df = create_joined_df(df, ever_df)\n",
    "    test_df = create_12_mon_features(joined_df)\n",
    "    joined_df = combine_joined_12_mon(joined_df, test_df)\n",
    "    del(test_df)\n",
    "    \n",
    "    perf_df = final_performance_delinquency(df, joined_df)\n",
    "    del(df, joined_df)\n",
    "    \n",
    "    final_df = join_perf_acq_gdfs(perf_df, acq_df)\n",
    "    del(perf_df)\n",
    "    del(acq_df)\n",
    "  \n",
    "    final_df = last_mile_cleaning(final_df)\n",
    "    return final_df\n",
    "\n",
    "def process_quarter_file(acquisition_file, perf_file):\n",
    "    print(\"Processing file: {0}\".format(perf_file))\n",
    "    \n",
    "    #Load Data\n",
    "    names_df = load_names(COL_NAMES_PATH)\n",
    "    acq_df = load_acquisition_csv(acquisition_file)\n",
    "    perf_df = load_performance_csv(perf_file)\n",
    "    \n",
    "    df = process_quarter(names_df, acq_df, perf_df)\n",
    "    del(acq_df, perf_df, names_df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dask_pandas_model_training'></a>\n",
    "### Model Training (Multi-core)\n",
    "CPU XGBoost using the Dask *train* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def dask_process_quarter(client, acquisition_file, perf_file):\n",
    "    print(\"Processing file: {0}\".format(perf_file))\n",
    "    #process_quarter_file(acquisition_file=acquisition_file,perf_file=perf_file)\n",
    "    task = dask_run_task(delayed(process_quarter_file),\n",
    "                                          acquisition_file=acquisition_file,\n",
    "                                          perf_file=perf_file)\n",
    "    return client.compute(task,\n",
    "                          optimize_graph=False,\n",
    "                          fifo_timeout=\"0ms\")\n",
    "\n",
    "# NOTE: The ETL calculates additional features which are then dropped before creating the XGBoost DMatrix.\n",
    "# This can be optimized to avoid calculating the dropped features.\n",
    "def dask_process_quarters(client, start_year, end_year):\n",
    "    frames = []\n",
    "    quarter = 1\n",
    "    year = start_year\n",
    "    while year != end_year:\n",
    "        acquisition_file = ACQ_DATA_PATH + \"/Acquisition_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n",
    "        files = glob(os.path.join(PERF_DATA_PATH + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\"))\n",
    "        for file in files:\n",
    "            delayed_df = dask_process_quarter(client, acquisition_file, file)\n",
    "            frames.append(delayed_df)\n",
    "                \n",
    "        quarter += 1\n",
    "        if quarter == 5:\n",
    "            year += 1\n",
    "            quarter = 1\n",
    "\n",
    "    wait(frames)\n",
    "    \n",
    "    print('Concatenating dataframes...')\n",
    "    frames = [delayed(pd.DataFrame)(df) for df in frames]\n",
    "    frames = [df for df in frames]\n",
    "    wait(frames)\n",
    "        \n",
    "    tmp_map = [(df, list(client.who_has(df).values())[0]) for df in frames]    \n",
    "    new_map = {}\n",
    "    for key, value in tmp_map:\n",
    "        if value not in new_map:\n",
    "            new_map[value] = [key]\n",
    "        else:\n",
    "            new_map[value].append(key)\n",
    "    del(tmp_map)\n",
    "    \n",
    "    dfs = []\n",
    "    for list_delayed in new_map.values():\n",
    "        dfs.append(delayed(pd.concat)(list_delayed))\n",
    "        \n",
    "    del(frames, new_map)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "client = dask_initialize(-1,0)\n",
    "df_train = dask_process_quarters(client, START_YEAR, END_YEAR)\n",
    "\n",
    "#Optional\n",
    "#print(\"Frame Count: {0}\".format(len(dfs_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#filter the data by the desired columns\n",
    "PRED_VARS_Y ='delinquency_12'\n",
    "\n",
    "dfs_xy = [(df[[PRED_VARS_Y]], df[delayed(list)(df.columns.difference([PRED_VARS_Y]))]) for df in df_train]\n",
    "dfs_xy = [(df[0].persist(), df[1].persist()) for df in dfs_xy]\n",
    "gc.collect()\n",
    "wait(dfs_xy)\n",
    "#del (df_train) #Comment out for PART_COUNT benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#PART_COUNT=4 #Override the global part_count to increase DMatrices count which increases the CPU cores\n",
    "\n",
    "#Add a rows_train calculation to the tuple\n",
    "dfs_xyr = [(df[0], df[1], dask.delayed(len)(df[1])) for df in dfs_xy]\n",
    "dfs_xyr = [(df[0].persist(), df[1].persist(), int(df[2].compute()*PERCENT_TRAIN)) for df in dfs_xyr]\n",
    "wait(dfs_xyr)\n",
    "del (dfs_xy)\n",
    "\n",
    "#Use PART_COUNT as a stride to partition the dataset for each GPU\n",
    "dfs_train = []\n",
    "for i in range(0,PART_COUNT):\n",
    "    dfs_train += ([(df[0][i:df[2]:PART_COUNT], df[1][i:df[2]:PART_COUNT]) for df in dfs_xyr])\n",
    "dfs_train = [(df[0].persist(), df[1].persist()) for df in dfs_train]\n",
    "\n",
    "#Use PART_COUNT as a stride to partition the dataset for each GPU\n",
    "dfs_test = []\n",
    "for i in range(0,PART_COUNT):\n",
    "    dfs_test += ([(df[0][df[2]+i::PART_COUNT], df[1][df[2]+i::PART_COUNT]) for df in dfs_xyr])\n",
    "dfs_test = [(df[0].persist(), df[1].persist()) for df in dfs_test]\n",
    "\n",
    "wait(dfs_train)\n",
    "wait(dfs_test)\n",
    "del(dfs_xyr)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#Optional: \n",
    "#rows_train = [dask.delayed(len)(df[1]) for df in dfs_train]\n",
    "#rows_train = [df.compute() for df in rows_train]\n",
    "#wait(rows_train)\n",
    "#print(rows_train)\n",
    "#rows_train_sum = sum(rows_train)\n",
    "#print(rows_train_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = [dask.delayed(xgb.DMatrix)(df[1].values, label=df[0].values, nthread=-1) for df in dfs_train]\n",
    "dtrain = [dmatrix.persist() for dmatrix in dtrain]\n",
    "wait(dtrain)\n",
    "\n",
    "del(dfs_train) \n",
    "gc.collect()\n",
    "\n",
    "#Optional\n",
    "print(\"DMatrix Count: {0}\".format(len(dtrain)))\n",
    "#print(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#The number of processing tasks determined by dask_initialize\n",
    "NTHREADS_ALWAYS_1_IN_DASK=1\n",
    "\n",
    "dxgb_params = {\n",
    "    'nround':            NUM_ROUNDS,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "#    'gamma':             0.1, #breaks the dask multi-core training wqith these parameters\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'hist',\n",
    "    'nthread':           NTHREADS_ALWAYS_1_IN_DASK,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    'objective':         'reg:linear',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "gbm = dxgb.train(client, dxgb_params,dtrain, labels=None,num_boost_round=NUM_ROUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy (AUC) is measured by the area under the ROC curve. The definition of an acceptable AUC is relative and not absolute based on the data and feature. An area of 1 represents a perfect test; an area of .5 represents a poor or unrealiable test. A suggested rough guide for classifying the accuracy of a XGBoost:<br>\n",
    ">90 - 100 = Excellent <br>\n",
    ">80 - 90 = Good <br>\n",
    ">70 - 80 = Fair <br>\n",
    ">60 - 70 = Poor <br>\n",
    ">50 - 60 = Fail <br>\n",
    ">50 and Below = Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#make some room in memory\n",
    "del dtrain\n",
    "\n",
    "#create the testing datasets for prediciotn and auc\n",
    "dtest = [dask.dataframe.from_delayed(df[1]) for df in dfs_test]\n",
    "dtest = [xgb.DMatrix(df.values, nthread=-1) for df in dtest]\n",
    "\n",
    "y_test = [dask.dataframe.from_delayed(df[0]) for df in dfs_test]\n",
    "y_test = [df.compute() for df in y_test]\n",
    "\n",
    "wait(y_test)\n",
    "wait(dtest)\n",
    "\n",
    "del(dfs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "y_row_count = float(sum([len(df) for df in y_test]))\n",
    "\n",
    "weighted_auc = 0\n",
    "weighted_roc = 0\n",
    "print(\"Partitions:\")\n",
    "for i in range(0,len(dtest)):\n",
    "    d = dtest[i]\n",
    "    y = y_test[i]\n",
    "    pred = gbm.predict(d)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    auc = metrics.auc(fpr, tpr) * 100\n",
    "    acc = metrics.roc_auc_score(y, pred) * 100\n",
    "\n",
    "    scale = float(len(y))/y_row_count\n",
    "    weighted_auc += auc * scale\n",
    "    weighted_roc += acc * scale\n",
    "    \n",
    "    print(\"{0}: AUC: {1}, ROC AUC: {2}\".format(i, auc, acc))\n",
    "\n",
    "print(\"======================================================\")\n",
    "print(\"Weighted: \")\n",
    "print(\"-- AUC: {0}, ROC AUC: {1}\\n\".format(weighted_auc, weighted_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup for the next iteration\n",
    "\n",
    "del dtrain\n",
    "del dtest\n",
    "del y_test\n",
    "del dxgb_params\n",
    "del gbm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cudf_dataframe'></a>\n",
    "## cuDF Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def cudf_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    return cudf.read_csv(performance_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def cudf_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    " \n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "    \n",
    "    return cudf.read_csv(acquisition_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def cudf_load_names(col_names_path, **kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "\n",
    "    #using categoricals to replace the string with a hashed int32\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    return cudf.read_csv(col_names_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL and Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cudf_null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column] = df[column].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def cudf_create_12_mon_features(joined_df, **kwargs):\n",
    "    result_dfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        temp_df = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        temp_df['temp_months'] = temp_df['timestamp_year'] * 12 + temp_df['timestamp_month']\n",
    "        temp_df['temp_mody_n'] = ((temp_df['temp_months'].astype('float64') - 24000 - y) / 12).floor()\n",
    "        temp_df = temp_df.groupby(['loan_id', 'temp_mody_n'], method='hash').agg({'delinquency_12': 'max','upb_12': 'min'})\n",
    "        temp_df['delinquency_12'] = (temp_df['max_delinquency_12']>3).astype('int32')\n",
    "        temp_df['delinquency_12'] +=(temp_df['min_upb_12']==0).astype('int32')\n",
    "        temp_df.drop_column('max_delinquency_12')\n",
    "        temp_df['upb_12'] = temp_df['min_upb_12']\n",
    "        temp_df.drop_column('min_upb_12')\n",
    "        temp_df['timestamp_year'] = (((temp_df['temp_mody_n'] * n_months) + 24000 + (y - 1)) / 12).floor().astype('int16')\n",
    "        temp_df['timestamp_month'] = np.int8(y)\n",
    "        temp_df.drop_column('temp_mody_n')\n",
    "        \n",
    "        result_dfs.append(temp_df)\n",
    "        del(temp_df)\n",
    "        \n",
    "    del(joined_df)\n",
    "    return cudf.concat(result_dfs)\n",
    "\n",
    "def cudf_create_ever_features(df, **kwargs):\n",
    "    ever_df = df[['loan_id', 'current_loan_delinquency_status']]\n",
    "    ever_df = ever_df.groupby('loan_id', method='hash').max()\n",
    "    del(df)\n",
    "    \n",
    "    ever_df['ever_30'] = (ever_df['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    ever_df['ever_90'] = (ever_df['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    ever_df['ever_180'] = (ever_df['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    ever_df.drop_column('max_current_loan_delinquency_status')\n",
    "    return ever_df\n",
    "\n",
    "def cudf_create_delinq_features(df, **kwargs):\n",
    "    delinq_df = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(df)\n",
    "    \n",
    "    delinq_30 = delinq_df.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop_column('min_monthly_reporting_period')\n",
    "    delinq_90 = delinq_df.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop_column('min_monthly_reporting_period')\n",
    "    delinq_180 = delinq_df.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop_column('min_monthly_reporting_period')\n",
    "    del(delinq_df)\n",
    "    \n",
    "    delinq_merge_df = delinq_30.merge(delinq_90, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge_df['delinquency_90'] = delinq_merge_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge_df = delinq_merge_df.merge(delinq_180, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge_df['delinquency_180'] = delinq_merge_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge_df\n",
    "\n",
    "def cudf_join_ever_delinq_features(df, delinq_df, **kwargs):\n",
    "    ever_df = df.merge(delinq_df, on=['loan_id'], how='left', type='hash')\n",
    "    del(df)\n",
    "    del(delinq_df)\n",
    "    \n",
    "    ever_df['delinquency_30'] = ever_df['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_90'] = ever_df['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    ever_df['delinquency_180'] = ever_df['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return ever_df\n",
    "\n",
    "def cudf_create_joined_df(df, ever_df, **kwargs):\n",
    "    test = df[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(df)\n",
    "    \n",
    "    test['timestamp'] = test['monthly_reporting_period']\n",
    "    test.drop_column('monthly_reporting_period')\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop_column('current_loan_delinquency_status')\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop_column('current_actual_upb')\n",
    "    test['upb_12'] = test['upb_12'].fillna(999999999)\n",
    "    test['delinquency_12'] = test['delinquency_12'].fillna(-1)\n",
    "    \n",
    "    joined_df = test.merge(ever_df, how='left', on=['loan_id'], type='hash')\n",
    "    del(ever_df)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'] = joined_df['ever_30'].fillna(-1)\n",
    "    joined_df['ever_90'] = joined_df['ever_90'].fillna(-1)\n",
    "    joined_df['ever_180'] = joined_df['ever_180'].fillna(-1)\n",
    "    joined_df['delinquency_30'] = joined_df['delinquency_30'].fillna(-1)\n",
    "    joined_df['delinquency_90'] = joined_df['delinquency_90'].fillna(-1)\n",
    "    joined_df['delinquency_180'] = joined_df['delinquency_180'].fillna(-1)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def cudf_combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop_column('delinquency_12')\n",
    "    joined_df.drop_column('upb_12')\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "\n",
    "def cudf_final_performance_delinquency(df, joined_df, **kwargs):\n",
    "    merged_df = cudf_null_workaround(df)\n",
    "    joined_df = cudf_null_workaround(joined_df)\n",
    "    merged_df['timestamp_month'] = merged_df['monthly_reporting_period'].dt.month\n",
    "    merged_df['timestamp_month'] = merged_df['timestamp_month'].astype('int8')\n",
    "    merged_df['timestamp_year'] = merged_df['monthly_reporting_period'].dt.year\n",
    "    merged_df['timestamp_year'] = merged_df['timestamp_year'].astype('int16')\n",
    "    merged_df = merged_df.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "    merged_df.drop_column('timestamp_year')\n",
    "    merged_df.drop_column('timestamp_month')\n",
    "    return merged_df\n",
    "\n",
    "def cudf_join_perf_acq_gdfs(perf_df, acq_df, **kwargs):\n",
    "    perf_df = cudf_null_workaround(perf_df)\n",
    "    acq_df = cudf_null_workaround(acq_df)\n",
    "    return perf_df.merge(acq_df, how='left', on=['loan_id'], type='hash')\n",
    "\n",
    "def cudf_last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    for column in drop_list:\n",
    "        df.drop_column(column)\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cudf_process_quarter(names_df, acq_df, perf_df):\n",
    "    acq_df = acq_df.merge(names_df, how='left', on=['seller_name'])\n",
    "    acq_df.drop_column('seller_name')\n",
    "    acq_df['seller_name'] = acq_df['new']\n",
    "    acq_df.drop_column('new')\n",
    "        \n",
    "    df = perf_df\n",
    "    ever_df = cudf_create_ever_features(df)\n",
    "    delinq_merge = cudf_create_delinq_features(df)\n",
    "    ever_df = cudf_join_ever_delinq_features(ever_df, delinq_merge)\n",
    "    del(delinq_merge)\n",
    "    \n",
    "    joined_df = cudf_create_joined_df(df, ever_df)\n",
    "    test_df = cudf_create_12_mon_features(joined_df)\n",
    "    joined_df = cudf_combine_joined_12_mon(joined_df, test_df)\n",
    "    del(test_df)\n",
    "    \n",
    "    perf_df = cudf_final_performance_delinquency(df, joined_df)\n",
    "    del(df, joined_df)\n",
    "    \n",
    "    final_df = cudf_join_perf_acq_gdfs(perf_df, acq_df)\n",
    "    del(perf_df)\n",
    "    del(acq_df)\n",
    "    \n",
    "    final_df = cudf_last_mile_cleaning(final_df)\n",
    "    return final_df\n",
    "\n",
    "def cudf_process_quarter_file(acquisition_file, perf_file):\n",
    "    #Load Data\n",
    "    names_df = cudf_load_names(COL_NAMES_PATH)\n",
    "    acq_df = cudf_load_acquisition_csv(acquisition_file)\n",
    "    perf_df = cudf_load_performance_csv(perf_file)\n",
    "    \n",
    "    df = cudf_process_quarter(names_df, acq_df, perf_df)\n",
    "    del(names_df, acq_df,perf_df)\n",
    "    \n",
    " #   return df.to_arrow(index=False)\n",
    "    return df.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dask_cudf_model_training'></a>\n",
    "### Model Training (Multi-GPU)\n",
    "GPU accelerated XGBoost using the Dask *train* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def dask_cudf_process_quarter(client, acquisition_file, perf_file):\n",
    "    print(\"Processing file: {0}\".format(perf_file))\n",
    "    #cudf_process_quarter_file(acquisition_file=acquisition_file,perf_file=perf_file)\n",
    "    task = dask_run_task(delayed(cudf_process_quarter_file),\n",
    "                                          acquisition_file=acquisition_file,\n",
    "                                          perf_file=perf_file)\n",
    "    return client.compute(task,\n",
    "                          optimize_graph=False,\n",
    "                          fifo_timeout=\"0ms\")\n",
    "\n",
    "# NOTE: The ETL calculates additional features which are then dropped before creating the XGBoost DMatrix.\n",
    "# This can be optimized to avoid calculating the dropped features.\n",
    "def dask_cudf_process_quarters(client, start_year, end_year):\n",
    "    frames = []\n",
    "    quarter = 1\n",
    "    year = start_year\n",
    "    while year != end_year:\n",
    "        acquisition_file = ACQ_DATA_PATH + \"/Acquisition_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n",
    "        files = glob(os.path.join(PERF_DATA_PATH + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\"))\n",
    "        for file in files:\n",
    "            delayed_df = dask_cudf_process_quarter(client, acquisition_file, file)\n",
    "            frames.append(delayed_df)\n",
    "                \n",
    "        quarter += 1\n",
    "        if quarter == 5:\n",
    "            year += 1\n",
    "            quarter = 1\n",
    "\n",
    "    wait(frames)\n",
    "    \n",
    "    print('Concatenating dataframes...')\n",
    "    frames = [delayed(pd.DataFrame)(df) for df in frames]\n",
    "    frames = [df for df in frames]\n",
    "    wait(frames)\n",
    "\n",
    "    tmp_map = [(df, list(client.who_has(df).values())[0]) for df in frames]\n",
    "    new_map = {}\n",
    "    for key, value in tmp_map:\n",
    "        if value not in new_map:\n",
    "            new_map[value] = [key]\n",
    "        else:\n",
    "            new_map[value].append(key)\n",
    "    del(tmp_map)\n",
    "\n",
    "    dfs = []\n",
    "    for list_delayed in new_map.values():\n",
    "#        dfs.append(delayed(cudf.concat)(list_delayed))\n",
    "        dfs.append(delayed(pd.concat)(list_delayed))\n",
    "        \n",
    "    del(frames, new_map)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "client = dask_initialize()\n",
    "client.run(rmm_initialize)\n",
    "\n",
    "df_train = dask_cudf_process_quarters(client, START_YEAR, END_YEAR)\n",
    "\n",
    "client.run(rmm_release)\n",
    "client.run(rmm_initialize_nopool)\n",
    "#Optional\n",
    "#print(\"Frame Count: {0}\".format(len(dfs_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#filter the data by the desired columns\n",
    "\n",
    "PRED_VARS_Y ='delinquency_12'\n",
    "\n",
    "dfs_xy = [(df[[PRED_VARS_Y]], df[delayed(list)(df.columns.difference([PRED_VARS_Y]))]) for df in df_train]\n",
    "dfs_xy = [(df[0].persist(), df[1].persist()) for df in dfs_xy]\n",
    "gc.collect()\n",
    "wait(dfs_xy)\n",
    "#del (dfs_train)  #Comment out for PART_COUNT benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#PART_COUNT = 1 #Override the global part_count to increase DMatrices count and reduce dataset size to match the gpu framebuffer size\n",
    "\n",
    "#Add a rows_train calculation to the tuple\n",
    "dfs_xyr = [(df[0], df[1], dask.delayed(len)(df[1])) for df in dfs_xy]\n",
    "dfs_xyr = [(df[0].persist(), df[1].persist(), int(df[2].compute()*PERCENT_TRAIN)) for df in dfs_xyr]\n",
    "wait(dfs_xyr)\n",
    "del (dfs_xy)\n",
    "\n",
    "#Use PART_COUNT as a stride to partition the dataset for each GPU\n",
    "dfs_train = []\n",
    "for i in range(0,PART_COUNT):\n",
    "    dfs_train += ([(df[0][i:df[2]:PART_COUNT], df[1][i:df[2]:PART_COUNT]) for df in dfs_xyr])\n",
    "dfs_train = [(df[0].persist(), df[1].persist()) for df in dfs_train]\n",
    "\n",
    "#Use PART_COUNT as a stride to partition the dataset for each GPU\n",
    "dfs_test = []\n",
    "for i in range(0,PART_COUNT):\n",
    "    dfs_test += ([(df[0][df[2]+i::PART_COUNT], df[1][df[2]+i::PART_COUNT]) for df in dfs_xyr])\n",
    "dfs_test = [(df[0].persist(), df[1].persist()) for df in dfs_test]\n",
    "\n",
    "wait(dfs_train)\n",
    "wait(dfs_test)\n",
    "del(dfs_xyr)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#Optional: \n",
    "#rows_train = [dask.delayed(len)(df[1]) for df in dfs_train]\n",
    "#rows_train = [df.compute() for df in rows_train]\n",
    "#wait(rows_train)\n",
    "#print(rows_train)\n",
    "#rows_train_sum = sum(rows_train)\n",
    "#print(rows_train_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = [dask.delayed(xgb.DMatrix)(df[1].values, label=df[0].values, nthread=-1) for df in dfs_train]\n",
    "dtrain = [dmatrix.persist() for dmatrix in dtrain]\n",
    "wait(dtrain)\n",
    "del(dfs_train)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#Optional\n",
    "print(\"DMatrix Count: {0}\".format(len(dtrain)))\n",
    "#print(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NTHREADS_ALWAYS_1_IN_DASK=1\n",
    "NGPUS_ALWAYS_1_IN_DASK=1\n",
    "\n",
    "dxgb_params = {\n",
    "    'nround':            NUM_ROUNDS,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'nthread':           NTHREADS_ALWAYS_1_IN_DASK,\n",
    "    'n_gpus':            NGPUS_ALWAYS_1_IN_DASK,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    'objective':         'gpu:reg:linear',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "gbm = dxgb.train(client, dxgb_params,dtrain, labels=None,num_boost_round=NUM_ROUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Accuracy (AUC) is measured by the area under the ROC curve. The definition of an acceptable AUC is relative and not absolute based on the data and feature. An area of 1 represents a perfect test; an area of .5 represents a poor or unrealiable test. A suggested rough guide for classifying the accuracy of a XGBoost:<br>\n",
    ">90 - 100 = Excellent <br>\n",
    ">80 - 90 = Good <br>\n",
    ">70 - 80 = Fair <br>\n",
    ">60 - 70 = Poor <br>\n",
    ">50 - 60 = Fail <br>\n",
    ">50 and Below = Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#make some room in memory\n",
    "del dtrain\n",
    "\n",
    "#create the testing datasets for prediciotn and auc\n",
    "dtest = [dask.dataframe.from_delayed(df[1]) for df in dfs_test]\n",
    "dtest = [xgb.DMatrix(df.values, nthread=-1) for df in dtest]\n",
    "\n",
    "y_test = [dask.dataframe.from_delayed(df[0]) for df in dfs_test]\n",
    "y_test = [df.compute() for df in y_test]\n",
    "\n",
    "wait(y_test)\n",
    "wait(dtest)\n",
    "\n",
    "del(dfs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "#Total row count in test dataset\n",
    "y_row_count = float(sum([len(df) for df in y_test]))\n",
    "\n",
    "weighted_auc = 0\n",
    "weighted_roc = 0\n",
    "print(\"Partitions:\")\n",
    "for i in range(0,len(dtest)):\n",
    "    d = dtest[i]\n",
    "    y = y_test[i]\n",
    "    pred = gbm.predict(d)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "    auc = metrics.auc(fpr, tpr) * 100\n",
    "    acc = metrics.roc_auc_score(y, pred) * 100\n",
    "\n",
    "    scale = float(len(y))/y_row_count\n",
    "    weighted_auc += auc * scale\n",
    "    weighted_roc += acc * scale\n",
    "    \n",
    "    print(\"{0}: AUC: {1}, ROC AUC: {2}\".format(i, auc, acc))\n",
    "\n",
    "print(\"======================================================\")\n",
    "print(\"Weighted: \")\n",
    "print(\"-- AUC: {0}, ROC AUC: {1}\\n\".format(weighted_auc, weighted_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup for the next iteration\n",
    "del dtest\n",
    "del y_test\n",
    "del dxgb_params\n",
    "del gbm\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(rmm_release)\n",
    "del client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleanup'></a>\n",
    "## Cleanup / Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
